# Version changelog

## 0.6.0

* Added method to dashboards to get dashboard url ([#211](https://github.com/databrickslabs/lsql/issues/211)). In this release, we have added a new method `get_url` to the `lakeview_dashboards` object in the `laksedashboard` library. This method utilizes the Databricks SDK to retrieve the dashboard URL, simplifying the code and making it more maintainable. Previously, the dashboard URL was constructed by concatenating the host and dashboard ID, but this new method ensures that the URL is obtained correctly, even if the format changes in the future. Additionally, a new unit test has been added for a method that gets the dashboard URL using the workspace client. This new functionality allows users to easily retrieve the URL for a dashboard using its ID and the workspace client.
* Extend replace database in query ([#210](https://github.com/databrickslabs/lsql/issues/210)). This commit extends the database replacement functionality in the `DashboardMetadata` class, allowing users to specify which database and catalog to replace. The enhancement includes support for catalog replacement and a new `replace_database` method in the `DashboardMetadata` class, which replaces the catalog and/or database in the query based on provided parameters. These changes enhance the flexibility and customization of the database replacement feature in queries, making it easier for users to control how their data is displayed in the dashboard. The `create_dashboard` function has also been updated to use the new method for replacing the database and catalog. Additionally, the `TileMetadata` update method has been replaced with a new merge method, and the `QueryTile` and `Tile` classes have new properties and methods for handling content, width, height, and position. The commit also includes several unit tests to ensure the new functionality works as expected.
* Improve object oriented dashboard-as-code implementation ([#208](https://github.com/databrickslabs/lsql/issues/208)). In this release, the object-oriented implementation of the dashboard-as-code feature has been significantly improved, addressing previous pull request comments ([#201](https://github.com/databrickslabs/lsql/issues/201)). The `TileMetadata` dataclass now includes methods for updating and comparing tile metadata, and the `DashboardMetadata` class has been removed and its functionality incorporated into the `Dashboards` class. The `Dashboards` class now generates tiles, datasets, and layouts for dashboards using the provided `query_transformer`. The code's readability and maintainability have been further enhanced by replacing the use of the `copy` module with `dataclasses.replace` for creating object copies. Additionally, updates have been made to the unit tests for dashboard functionality in the project, with new methods and attributes added to check for valid dashboard metadata and handle duplicate query or widget IDs, as well as to specify the order in which tiles and widgets should be displayed in the dashboard.


## 0.5.0

* Added Command Execution backend which uses Command Execution API on a cluster ([#95](https://github.com/databrickslabs/lsql/issues/95)). In this release, the databricks labs lSQL library has been updated with a new Command Execution backend that utilizes the Command Execution API. A new `CommandExecutionBackend` class has been implemented, which initializes a `CommandExecutor` instance taking a cluster ID, workspace client, and language as parameters. The `execute` method runs SQL commands on the specified cluster, and the `fetch` method returns the query result as an iterator of Row objects. The existing `StatementExecutionBackend` class has been updated to inherit from a new abstract base class called `ExecutionBackend`, which includes a `save_table` method for saving data to tables and is meant to be a common base class for both Statement and Command Execution backends. The `StatementExecutionBackend` class has also been updated to use the new `ExecutionBackend` abstract class and its constructor now accepts a `max_records_per_batch` parameter. The `execute` and `fetch` methods have been updated to use the new `_only_n_bytes` method for logging truncated SQL statements. Additionally, the `CommandExecutionBackend` class has several methods, `execute`, `fetch`, and `save_table` to execute commands on a cluster and save the results to tables in the databricks workspace. This new backend is intended to be used for executing commands on a cluster and saving the results in a databricks workspace.
* Added basic integration with Lakeview Dashboards ([#66](https://github.com/databrickslabs/lsql/issues/66)). In this release, we've added basic integration with Lakeview Dashboards to the project, enhancing its capabilities. This includes updating the `databricks-labs-blueprint` dependency to version 0.4.2 with the `[yaml]` extra, allowing for additional functionality related to handling YAML files. A new file, `dashboards.py`, has been introduced, providing a class for interacting with Databricks dashboards, along with methods for retrieving and saving dashboard configurations. Additionally, a new `__init__.py` file under the `src/databricks/labs/lsql/lakeview` directory imports all classes and functions from the `model.py` module, providing a foundation for further development and customization. The release also introduces a new file, `model.py`, containing code generated from OpenAPI specs by the Databricks SDK Generator, and a template file, `model.py.tmpl`, used for handling JSON data during integration with Lakeview Dashboards. A new file, `polymorphism.py`, provides utilities for checking if a value can be assigned to a specific type, supporting correct data typing and formatting with Lakeview Dashboards. Furthermore, a `.gitignore` file has been added to the `tests/integration` directory as part of the initial steps in adding integration testing to ensure compatibility with the Lakeview Dashboards platform. Lastly, the `test_dashboards.py` file in the `tests/integration` directory contains a function, `test_load_dashboard(ws)`, which uses the `Dashboards` class to save a dashboard from a source to a destination path, facilitating testing during the integration process.
* Added dashboard-as-code functionality ([#201](https://github.com/databrickslabs/lsql/issues/201)). This commit introduces dashboard-as-code functionality for the UCX project, enabling the creation and management of dashboards using code. The feature resolves multiple issues and includes a new `create-dashboard` command for creating unpublished dashboards. The functionality is available in the `lsql` lab and allows for specifying the order and width of widgets, overriding default widget identifiers, and supporting various SQL and markdown header arguments. The `dashboard.yml` file is used to define top-level metadata for the dashboard. This commit also includes extensive documentation and examples for using the dashboard as a library and configuring different options.
* Automate opening integration test dashboard in debug mode ([#167](https://github.com/databrickslabs/lsql/issues/167)). A new feature has been added to automatically open the integration test dashboard in debug mode, making it easier for software engineers to debug and troubleshoot. This has been achieved by importing the `webbrowser` and `is_in_debug` modules from "databricks.labs.blueprint.entrypoint", and adding a check in the `create` function to determine if the code is running in debug mode. If it is, a dashboard URL is constructed from the workspace configuration and dashboard ID, and then opened in a web browser using "webbrowser.open". This allows for a more streamlined debugging process for the integration test dashboard. No other parts of the code have been affected by this change.
* Automatically tile widgets ([#109](https://github.com/databrickslabs/lsql/issues/109)). In this release, we've introduced an automatic widget tiling feature for the dashboard creation process in our open-source library. The `Dashboards` class now includes a new class variable, `_maximum_dashboard_width`, set to 6, representing the maximum width allowed for each row of widgets in the dashboard. The `create_dashboard` method has been updated to accept a new `self` parameter, turning it into an instance method. A new `_get_position` method has been introduced to calculate and return the next available position for placing a widget, and a `_get_width_and_height` method has been added to return the width and height for a widget specification, initially handling `CounterSpec` instances. Additionally, we've added new unit tests to improve testing coverage, ensuring that widgets are created, positioned, and sized correctly. These tests also cover the correct positioning of widgets based on their order and available space, as well as the expected width and height for each widget.
* Bump actions/checkout from 4.1.3 to 4.1.6 ([#102](https://github.com/databrickslabs/lsql/issues/102)). In the latest release, the 'actions/checkout' GitHub Action has been updated from version 4.1.3 to 4.1.6, which includes checking the platform to set the archive extension appropriately. This release also bumps the version of github/codeql-action from 2 to 3, actions/setup-node from 1 to 4, and actions/upload-artifact from 2 to 4. Additionally, the minor-actions-dependencies group was updated with two new versions. Disabling extensions.worktreeConfig when disabling sparse-checkout was introduced in version 4.1.4. The release notes and changelog for this update can be found in the provided link. This commit was made by dependabot[bot] with contributions from cory-miller and jww3.
* Bump actions/checkout from 4.1.6 to 4.1.7 ([#151](https://github.com/databrickslabs/lsql/issues/151)). In the latest release, the 'actions/checkout' GitHub action has been updated from version 4.1.6 to 4.1.7 in the project's push workflow, which checks out the repository at the start of the workflow. This change brings potential bug fixes, performance improvements, or new features compared to the previous version. The update only affects the version number in the YAML configuration for the 'actions/checkout' step in the release.yml file, with no new methods or alterations to existing functionality. This update aims to ensure a smooth and enhanced user experience for those utilizing the project's push workflows by taking advantage of the possible improvements or bug fixes in the new version of 'actions/checkout'.
* Create a dashboard with a counter from a single query ([#107](https://github.com/databrickslabs/lsql/issues/107)). In this release, we have introduced several enhancements to our dashboard-as-code approach, including the creation of a `Dashboards` class that provides methods for getting, saving, and deploying dashboards. A new method, `create_dashboard`, has been added to create a dashboard with a single page containing a counter widget. The counter widget is associated with a query that counts the number of rows in a specified dataset. The `deploy_dashboard` method has also been added to deploy the dashboard to the workspace. Additionally, we have implemented a new feature for creating dashboards with a counter from a single query, including modifications to the `test_dashboards.py` file and the addition of four new tests. These changes improve the robustness of the dashboard creation process and provide a more automated way to view important metrics.
* Create text widget from markdown file ([#142](https://github.com/databrickslabs/lsql/issues/142)). A new feature has been implemented in the library that allows for the creation of a text widget from a markdown file, enhancing customization and readability for users. This development resolves issue [#1](https://github.com/databrickslabs/lsql/issues/1)
* Design document for dashboards-as-code ([#105](https://github.com/databrickslabs/lsql/issues/105)). "The latest release introduces 'Dashboards as Code,' a method for defining and managing dashboards through configuration files, enabling version control and controlled changes. The building blocks include `.sql`, `.md`, and `dashboard.yml` files, with `.sql` defining queries and determining tile order, and `dashboard.yml` specifying top-level metadata and tile overrides. Metadata can be inferred or explicitly defined in the query or files. The tile order can be determined by SQL file order, `tiles` order in `dashboard.yml`, or SQL file metadata. This project can also be used as a library for embedding dashboard generation in your code. Configuration precedence follows command-line flags, SQL file headers, `dashboard.yml`, and SQL query content. The command-line interface is utilized for dashboard generation from configuration files."
* Ensure propagation of `lsql` version into `User-Agent` header when it is used as library ([#206](https://github.com/databrickslabs/lsql/issues/206)). In this release, the `pyproject.toml` file has been updated to ensure that the correct version of the `lsql` library is propagated into the `User-Agent` header when used as a library, improving attribution. The `databricks-sdk` version has been updated from `0.22.0` to `0.29.0`, and the `__init__.py` file of the `lsql` library has been modified to add the `with_user_agent_extra` function from the `databricks.sdk.core` package for correct attribution. The `backends.py` file has also been updated with improved type handling in the `_row_to_sql` and `save_table` functions for accurate SQL insertion and handling of user-defined classes. Additionally, a test has been added to ensure that the `lsql` version is correctly propagated in the `User-Agent` header when used as a library. These changes offer improved functionality and accurate type handling, making it easier for developers to identify the library version when used in other projects.
* Fixed counter encodings ([#143](https://github.com/databrickslabs/lsql/issues/143)). In this release, we have improved the encoding of counters in the lsql dashboard by modifying the `create_dashboard` function in the `dashboards.py` file. Previously, the counter field encoding was hardcoded as "count," but has been changed to dynamically determine the first field name of the given fields, ensuring that counters are expected to have only one field. Additionally, a new integration test has been added to the `tests/integration/test_dashboards.py` file to ensure that the dashboard deployment functionality correctly handles SQL queries that do not perform a count. A new test for the `Dashboards` class has also been added to check that counter field encoding names are created as expected. The `WorkspaceClient` is mocked and not called in this test. These changes enhance the accuracy of counter encoding and improve the overall functionality and reliability of the lsql dashboard.
* Fixed non-existing reference and typo in the documentation ([#104](https://github.com/databrickslabs/lsql/issues/104)). In this release, we've made improvements to the documentation of our open-source library, specifically addressing issue [#104](https://github.com/databrickslabs/lsql/issues/104). The changes include fixing a non-existent reference and a typo in the `Library size comparison` section of the "comparison.md" document. This section provides guidance for selecting a library based on factors like library size, unified authentication, and compatibility with various Databricks warehouses and SQL Python APIs. The updates clarify the required dependency size for simple applications and scripts, and offer more detailed information about each library option. We've also added a new subsection titled `Detailed comparison` to provide a more comprehensive overview of each library's features. These changes are intended to help software engineers better understand which library is best suited for their specific needs, particularly for applications that require data transfer of large amounts of data serialized in Apache Arrow format and low result fetching latency, where we recommend using the Databricks SQL Connector for Python for efficient data transfer and low latency.
* Fixed parsing message ([#146](https://github.com/databrickslabs/lsql/issues/146)). In this release, the warning message logged during the creation of a dashboard when a ParseError occurs has been updated to provide clearer and more detailed information about the parsing error. The new error message now includes the specific query being parsed and the exact parsing error, enabling developers to quickly identify the cause of parsing issues. This change ensures that engineers can efficiently diagnose and address parsing errors, improving the overall development and debugging experience with a more informative log format: "Parsing {query}: {error}".
* Improve dashboard as code ([#108](https://github.com/databrickslabs/lsql/issues/108)). The `Dashboards` class in the 'dashboards.py' file has been updated to improve functionality and usability, with changes such as the addition of a type variable `T` for type checking and more descriptive names for methods. The `save_to_folder` method now accepts a `Dashboard` object and returns a `Dashboard` object, and a new static method `create_dashboard` has been added. Additionally, two new methods `_with_better_names` and `_replace_names` have been added for improved readability. The `get_dashboard` method now returns a `Dashboard` object instead of a dictionary. The `save_to_folder` method now also formats SQL code before saving it to file. These changes aim to enhance the functionality and readability of the codebase and provide more user-friendly methods for interacting with the `Dashboards` class. In addition to the changes in the `Dashboards` class, there have been updates in the organization of the project structure. The 'queries/counter.sql' file has been moved to 'dashboards/one_counter/counter.sql' in the 'tests/integration' directory. This modification enhances the organization of the project. Furthermore, several tests for the `Dashboards` class have been introduced in the 'databricks.labs.lsql.dashboards' module, demonstrating various functionalities of the class and ensuring that it functions as intended. The tests cover saving SQL and YML files to a specified folder, creating a dataset and a counter widget for each query, deploying dashboards with a given display name or dashboard ID, and testing the behavior of the `save_to_folder` and `deploy_dashboard` methods. Lastly, the commit removes the `test_load_dashboard` function and updates the `test_dashboard_creates_one_dataset_per_query` and `test_dashboard_creates_one_counter_widget_per_query` functions to use the updated `Dashboard` class. A new `replace_recursively` function is introduced to replace specific fields in a dataclass recursively. A new test function `test_dashboards_deploys_exported_dashboard_definition` has been added, which reads a dashboard definition from a JSON file, deploys it, and checks if it's successfully deployed using the `Dashboards` class. A new test function `test_dashboard_deploys_dashboard_the_same_as_created_dashboard` has also been added, which compares the original and deployed dashboards to ensure they are identical. Overall, these changes aim to improve the functionality and readability of the codebase and provide more user-friendly methods for interacting with the `Dashboards` class, as well as enhance the organization of the project structure and add new tests for the `Dashboards` class to ensure it functions as intended.
* Infer fields from a query ([#111](https://github.com/databrickslabs/lsql/issues/111)). The `Dashboards` class in the `dashboards.py` file has been updated with the addition of a new method, `_get_fields`, which accepts a SQL query as input and returns a list of `Field` objects using the `sqlglot` library to parse the query and extract the necessary information. The `create_dashboard` method has been modified to call this new function when creating `Query` objects for each dataset. If a `ParseError` occurs, a warning is logged and iteration continues. This allows for the automatic population of fields when creating a new dashboard, eliminating the need for manual specification. Additionally, new tests have been added for invalid queries and for checking if the fields in a query have the expected names. These tests include `test_dashboards_skips_invalid_query` and `test_dashboards_gets_fields_with_expected_names`, which utilize the caplog fixture and create temporary query files to verify functionality. Existing functionality related to creating dashboards remains unchanged.
* Make constant all caps ([#140](https://github.com/databrickslabs/lsql/issues/140)). In this release, the project's 'dashboards.py' file has been updated to improve code readability and maintainability. A constant variable `_maximum_dashboard_width` has been changed to all caps, becoming '_MAXIMUM_DASHBOARD_WIDTH'. This modification affects the `Dashboards` class and its methods, particularly `_get_fields` and '_get_position'. The `_get_position` method has been revised to use the new all caps constant variable. This change ensures better visibility of constants within the code, addressing issue [#140](https://github.com/databrickslabs/lsql/issues/140). It's important to note that this modification only impacts the 'dashboards.py' file and does not affect any other functionalities.
* Read display name from `dashboard.yml` ([#144](https://github.com/databrickslabs/lsql/issues/144)). In this release, we have introduced a new `DashboardMetadata` dataclass that reads the display name of a dashboard from a `dashboard.yml` file located in the dashboard's directory. If the `dashboard.yml` file is absent, the folder name will be used as the display name. This change improves the readability and maintainability of the dashboard configuration by explicitly defining the display name and reducing the need to specify widget information in multiple places. We have also added a new fixture called `make_dashboard` for creating and cleaning up lakeview dashboards in the test suite. The fixture handles creation and deletion of the dashboard and provides an option to set a custom display name. Additionally, we have added and modified several unit tests to ensure the proper handling of the `DashboardMetadata` class and the dashboard creation process, including tests for missing, present, or incorrect `display_name` keys in the YAML file. The `dashboards.deploy_dashboard()` function has been updated to handle cases where only `dashboard_id` is provided.
* Set widget id in query header ([#154](https://github.com/databrickslabs/lsql/issues/154)). In this release, we've made significant improvements to widget metadata handling in our open-source library. We've introduced a new `WidgetMetadata` class that replaces the previous `WidgetMetadata` dataclass, now featuring a `path` attribute, `spec_type` property, and optional parameters for `order`, `width`, `height`, and `_id`. The `_get_widgets` method has been updated to accept an Iterable of `WidgetMetadata` objects, and both `_get_layouts` and `_get_widgets` methods now sort widgets using the order field. A new class method, `WidgetMetadata.from_path`, handles parsing widget metadata from a file path, replacing the removed `_get_width_and_height` method. Additionally, the `WidgetMetadata` class is now used in the `deploy_dashboard` method, and the test suite for the `dashboards` module has been enhanced with updated `test_widget_metadata_replaces_width_and_height` and `test_widget_metadata_replaces_attribute` functions, as well as new tests for specific scenarios. Issue [#154](https://github.com/databrickslabs/lsql/issues/154) has been addressed by setting the widget id in the query header, and the aforementioned changes improve flexibility and ease of use for dashboard development.
* Use order key in query header if defined ([#149](https://github.com/databrickslabs/lsql/issues/149)). In this release, we've introduced a new feature to use an order key in the query header if defined, enhancing the flexibility and control over the dashboard creation process. The `WidgetMetadata` dataclass now includes an optional `order` parameter of type `int`, and the `_get_arguments_parser()` method accepts the `--order` flag with type `int`. The `replace_from_arguments()` method has been updated to support the new `order` parameter, with a default value of `self.order`. The `create_dashboard()` method now implements a new `_get_datasets()` method to retrieve datasets from the dashboard folder and introduces a `_get_widgets()` method, which accepts a list of files, iterates over them, and yields tuples containing widgets and their corresponding metadata, including the order. These improvements enable the use of an order key in query headers, ensuring the correct order of widgets in the dashboard creation process. Additionally, a new test case has been added to verify the correct behavior of the dashboard deployment with a specified order key in the query header. This feature resolves issue [#148](https://github.com/databrickslabs/lsql/issues/148).
* Use widget width and height defined in query header ([#147](https://github.com/databrickslabs/lsql/issues/147)). In this release, the handling of metadata in SQL files has been updated to utilize the header of the file, instead of the first line, for improved readability and flexibility. This change includes a new WidgetMetadata class for defining the width and height of a widget in a dashboard, as well as new methods for parsing the widget metadata from a provided path. The release also includes updates to the documentation to cover the supported widget arguments `-w or --width` and '-h or --height', and resolves issue [#114](https://github.com/databrickslabs/lsql/issues/114) by adding a test for deploying a dashboard with a big widget using a new function `test_dashboard_deploys_dashboard_with_big_widget`. Additionally, new test cases have been added for creating dashboards with custom-sized widgets based on query header width and height values, improving functionality and error handling.

Dependency updates:

 * Bump actions/checkout from 4.1.3 to 4.1.6 ([#102](https://github.com/databrickslabs/lsql/pull/102)).
 * Bump actions/checkout from 4.1.6 to 4.1.7 ([#151](https://github.com/databrickslabs/lsql/pull/151)).

## 0.4.3

* Bump actions/checkout from 4.1.2 to 4.1.3 ([#97](https://github.com/databrickslabs/lsql/issues/97)). The `actions/checkout` dependency has been updated from version 4.1.2 to 4.1.3 in the `update-main-version.yml` file. This new version includes a check to verify the git version before attempting to disable `sparse-checkout`, and adds an SSH user parameter to improve functionality and compatibility. The release notes and CHANGELOG.md file provide detailed information on the specific changes and improvements. The pull request also includes a detailed commit history and links to corresponding issues and pull requests on GitHub for transparency. You can review and merge the pull request to update the `actions/checkout` dependency in your project.
* Maintain PySpark compatibility for databricks.labs.lsql.core.Row ([#99](https://github.com/databrickslabs/lsql/issues/99)). In this release, we have added a new method `asDict` to the `Row` class in the `databricks.labs.lsql.core` module to maintain compatibility with PySpark. This method returns a dictionary representation of the `Row` object, with keys corresponding to column names and values corresponding to the values in each column. Additionally, we have modified the `fetch` function in the `backends.py` file to return `Row` objects of `pyspark.sql` when using `self._spark.sql(sql).collect()`. This change is temporary and marked with a `TODO` comment, indicating that it will be addressed in the future. We have also added error handling code in the `fetch` function to ensure the function operates as expected. The `asDict` method in this implementation simply calls the existing `as_dict` method, meaning the behavior of the `asDict` method is identical to the `as_dict` method. The `as_dict` method returns a dictionary representation of the `Row` object, with keys corresponding to column names and values corresponding to the values in each column. The optional `recursive` argument in the `asDict` method, when set to `True`, enables recursive conversion of nested `Row` objects to nested dictionaries. However, this behavior is not currently implemented, and the `recursive` argument is always `False` by default.

Dependency updates:

 * Bump actions/checkout from 4.1.2 to 4.1.3 ([#97](https://github.com/databrickslabs/lsql/pull/97)).

## 0.4.2

* Added more `NotFound` error type ([#94](https://github.com/databrickslabs/lsql/issues/94)). In the latest update, the `core.py` file in the `databricks/labs/lsql` package has undergone enhancements to the error handling functionality. The `_raise_if_needed` function has been modified to raise a `NotFound` error when the error message includes the phrase "does not exist". This update enables the system to categorize specific SQL query errors as `NotFound` error messages, thereby improving the overall error handling and reporting capabilities. This change was a collaborative effort, as indicated by the co-authored-by statement in the commit.


## 0.4.1

* Fixing ovewrite integration tests ([#92](https://github.com/databrickslabs/lsql/issues/92)). A new enhancement has been implemented for the `overwrite` feature's integration tests, addressing a concern with write operations. Two new variables, `catalog` and "schema", have been incorporated using the `env_or_skip` function. These variables are utilized in the `save_table` method, which is now invoked twice with the same table, once with the `append` and once with the `overwrite` option. The data in the table is retrieved and checked for accuracy after each call, employing the updated `Row` class with revised field names `first` and "second", formerly `name` and "id". This modification ensures the proper operation of the `overwrite` feature during integration tests and resolves any related issues. The commit message `Fixing overwrite integration tests` signifies this change.


## 0.4.0

* Added catalog and schema parameters to execute and fetch ([#90](https://github.com/databrickslabs/lsql/issues/90)). In this release, we have added optional `catalog` and `schema` parameters to the `execute` and `fetch` methods in the `SqlBackend` abstract base class, allowing for more flexibility when executing SQL statements in specific catalogs and schemas. These updates include new method signatures and their respective implementations in the `SparkSqlBackend` and `DatabricksSqlBackend` classes. The new parameters control the catalog and schema used by the `SparkSession` instance in the `SparkSqlBackend` class and the `SqlClient` instance in the `DatabricksSqlBackend` class. This enhancement enables better functionality in multi-catalog and multi-schema environments. Additionally, this change comes with unit tests and integration tests to ensure proper functionality. The new parameters can be used when calling the `execute` and `fetch` methods. For example, with a `SparkSqlBackend` instance `spark_backend`, you can execute a SQL statement in a specific catalog and schema with the following code: `spark_backend.execute("SELECT * FROM my_table", catalog="my_catalog", schema="my_schema")`. Similarly, the `fetch` method can also be used with the new parameters.


## 0.3.1

* Check UCX and LSQL for backwards compatibility ([#78](https://github.com/databrickslabs/lsql/issues/78)). In this release, we introduce a new GitHub Actions workflow, downstreams.yml, which automates unit testing for downstream projects upon changes made to the upstream project. The workflow runs on pull requests, merge groups, and pushes to the main branch and sets permissions for id-token, contents, and pull-requests. It includes a compatibility job that runs on Ubuntu, checks out the code, sets up Python, installs the toolchain, and accepts downstream projects using the databrickslabs/sandbox/downstreams action. The job matrix includes two downstream projects, ucx and remorph, and uses the build cache to speed up the pip install step. This feature ensures that changes to the upstream project do not break compatibility with downstream projects, maintaining a stable and reliable library for software engineers.
* Fixed `Builder` object has no attribute `sdk_config` error ([#86](https://github.com/databrickslabs/lsql/issues/86)). In this release, we've resolved a `Builder` object has no attribute `sdk_config` error that occurred when initializing a Spark session using the `DatabricksSession.builder` method. The issue was caused by using dot notation to access the `sdk_config` attribute, which is incorrect. This has been updated to the correct syntax of `sdkConfig`. This change enables successful creation of the Spark session, preventing the error from recurring. The `DatabricksSession` class and its methods, such as `getOrCreate`, continue to be used for interacting with Databricks clusters and workspaces, while the `WorkspaceClient` class manages Databricks resources within a workspace.

Dependency updates:

 * Bump codecov/codecov-action from 1 to 4 ([#84](https://github.com/databrickslabs/lsql/pull/84)).
 * Bump actions/setup-python from 4 to 5 ([#83](https://github.com/databrickslabs/lsql/pull/83)).
 * Bump actions/checkout from 2.5.0 to 4.1.2 ([#81](https://github.com/databrickslabs/lsql/pull/81)).
 * Bump softprops/action-gh-release from 1 to 2 ([#80](https://github.com/databrickslabs/lsql/pull/80)).

## 0.3.0

* Added support for `save_table(..., mode="overwrite")` to `StatementExecutionBackend` ([#74](https://github.com/databrickslabs/lsql/issues/74)). In this release, we've added support for overwriting a table when saving data using the `save_table` method in the `StatementExecutionBackend`. Previously, attempting to use the `overwrite` mode would raise a `NotImplementedError`. Now, when this mode is specified, the method first truncates the table before inserting the new rows. The truncation is done using the `execute` method to run a `TRUNCATE TABLE` SQL command. Additionally, we've added a new integration test, `test_overwrite`, to the `test_deployment.py` file to verify the new `overwrite` mode functionality. A new option, `mode="overwrite"`, has been added to the `save_table` method, allowing for the existing data in the table to be deleted and replaced with the new data being written. We've also added two new test cases, `test_statement_execution_backend_save_table_overwrite_empty_table` and `test_mock_backend_overwrite`, to verify the new functionality. It's important to note that the method signature has been updated to include a default value for the `mode` parameter, setting it to `append` by default. This change does not affect the functionality and only provides a more convenient default behavior for users of the method.


## 0.2.5

* Fixed PyPI badge ([#72](https://github.com/databrickslabs/lsql/issues/72)). In this release, we have implemented a fix to the PyPI badge in the README file of our open-source library. The PyPI badge displays the version of the package and serves as a quick reference for users. This fix ensures the accuracy and proper functioning of the badge, without involving any changes to the functionality or methods within the project. Software engineers can be assured that this update is limited to the README file, specifically the PyPI badge, and will not affect the overall functionality of the library.
* Fixed `no-cheat` check ([#71](https://github.com/databrickslabs/lsql/issues/71)). In this release, we have made improvements to the `no-cheat` verification process for new code. Previously, the check for disabling the linter was prone to false positives when the string '# pylint: disable' appeared for reasons other than disabling the linter. The updated code now includes an additional filter to exclude the string `CHEAT` from the search, and the number of characters in the output is counted using the `wc -c` command. If the count is not zero, the script will terminate with an error message. This change enhances the accuracy of the `no-cheat` check, ensuring that the linter is being used correctly and that all new code meets our quality standards.
* Removed upper bound on `sqlglot` dependency ([#70](https://github.com/databrickslabs/lsql/issues/70)). In this update, we have removed the upper bound on the `sqlglot` dependency version in the project's `pyproject.toml` file. Previously, the version constraint required `sqlglot` to be at least 22.3.1 but less than 22.5.0. With this modification, there will be no upper limit, enabling the project to utilize any version greater than or equal to 22.3.1. This change provides the project with the flexibility to take advantage of future bug fixes, performance improvements, and new features available in newer `sqlglot` package versions. Developers should thoroughly test the updated package version to ensure compatibility with the existing codebase.


## 0.2.4

* Fixed `Builder` object is not callable error ([#67](https://github.com/databrickslabs/lsql/issues/67)). In this release, we have made an enhancement to the `Backends` class in the `databricks/labs/lsql/backends.py` file. The `DatabricksSession.builder()` method call in the `__init__` method has been changed to `DatabricksSession.builder`. This update uses the `builder` attribute to create a new instance of `DatabricksSession` without calling it like a function. The `sdk_config` method is then used to configure the instance with the required settings. Finally, the `getOrCreate` method is utilized to obtain a `SparkSession` object, which is then passed as a parameter to the parent class constructor. This modification simplifies the code and eliminates the error caused by treating the `builder` attribute as a callable object. Software engineers may benefit from this change by having a more streamlined and error-free codebase when working with the open-source library.
* Prevent silencing of `pylint` ([#65](https://github.com/databrickslabs/lsql/issues/65)). In this release, we have introduced a new job, "no-lint-disabled", to the GitHub Actions workflow for the repository. This job runs on the latest Ubuntu version and checks out the codebase with a full history. It verifies that no new instances of code suppressing `pylint` checks have been added, by filtering the differences between the current branch and the main branch for new lines of code, and then checking if any of those new lines contain a `pylint` disable comment. If any such lines are found, the job will fail and print a message indicating the offending lines of code, thereby ensuring that the codebase maintains a consistent level of quality by not allowing linting checks to be bypassed.
* Updated `_SparkBackend.fetch()` to return iterator instead of list ([#62](https://github.com/databrickslabs/lsql/issues/62)). In this release, the `fetch()` method of the `_SparkBackend` class has been updated to return an iterator instead of a list, which can result in reduced memory usage and improved performance, as the results of the SQL query can now be processed one element at a time. A new exception has been introduced to wrap any exceptions that occur during query execution, providing better debugging and error handling capabilities. The `test_runtime_backend_fetch()` unit test has been updated to reflect this change, and users of the `fetch()` method should be aware that it now returns an iterator and must be consumed to obtain the desired data. Thorough testing is recommended to ensure that the updated method still meets the needs of the application.


## 0.2.3

* Added support for common parameters in StatementExecutionBackend ([#59](https://github.com/databrickslabs/lsql/issues/59)). The `StatementExecutionBackend` class in the `databricks.labs.lsql` package's `backends.py` file now supports the passing of common parameters through keyword arguments (kwargs). This enhancement allows for greater customization and flexibility in the backend's operation, as the kwargs are passed to the `StatementExecutionExt` constructor. This change empowers users to control the behavior of the backend, making it more adaptable to various use cases. The key modification in this commit is the addition of the `**kwargs` parameter in the constructor signature and passing it to `StatementExecutionExt`, with no changes made to any methods within the class.


## 0.2.2

* Updating packages. In this update, the dependencies specified in the pyproject.toml file have been updated to more recent versions. The outdated packages "databricks-labs-blueprint~=0.4.0" and "databricks-sdk~=0.21.0" have been replaced with "databricks-labs-blueprint>=0.4.2" and "databricks-sdk>=0.22.0", respectively. These updates are expected to bring new features and bug fixes to the software. The dependency `sqlglot` remains unchanged, with the same version requirement range of "sqlglot>=22.3.1,<22.5.0". These updates ensure that the software will function as intended, while also taking advantage of the enhancements provided by the more recent versions of the packages.


## 0.2.1

* Fixed row converter to properly handle nullable values ([#53](https://github.com/databrickslabs/lsql/issues/53)). In this release, the row converter in the `databricks.labs.lsql.core` module has been updated to handle nullable values correctly. A new method `StatementExecutionExt` has been added, which manages the handling of nullable values during SQL statement execution. The `Row` class has also been modified to include nullable values, improving the robustness and flexibility of SQL execution in dealing with various data types, including null values. These enhancements increase the overall reliability of the system, making it more production-ready.
* Improved integration test coverage ([#52](https://github.com/databrickslabs/lsql/issues/52)). In this release, the project's integration test coverage has been significantly improved through several changes. A new function, `make_random()`, has been added to the `conftest.py` file to generate a random string of fixed length, aiding in the creation of more meaningful and readable random strings for integration tests. A new file, `test_deployment.py`, has been introduced, containing a test function for deploying a database schema and verifying successful data retrieval via a view. The `test_integration.py` file has been renamed to `test_core.py`, with updates to the `test_fetch_one` function to test the `fetch_one` method using a SQL query with an aliased value. Additionally, a new `Foo` dataclass has been added to the `tests/integration/views/__init__.py` file, supporting integration test coverage. Lastly, a new SQL query has been added to the integration test suite, located in the `some.sql` file, which retrieves data from a table named `foo` in the `inventory` schema. These changes aim to enhance the overall stability, reliability, and coverage of the project's integration tests. Note: The changes to the `.gitignore` file and the improvements to the `StatementExecutionBackend` class in the `backends.py` file are not included in this summary, as they were described in the opening statement.
* Rely on `hatch` being present on the build machine ([#54](https://github.com/databrickslabs/lsql/issues/54)). In this release, we have made significant changes to how we manage our build process and toolchain configuration. We have removed the need to manually install `hatch` version 1.7.0 in the build machine, and instead, rely on its presence, adding it to the list of required tools in the toolchain configuration. The command to create a virtual environment using `hatch` has also been added, and the `pre_setup` section no longer includes installing `hatch`, assuming its availability. We have also updated the `hatch` package version from 1.7.0 to 1.9.4, which may include bug fixes, performance improvements, or new features. This change may impact the behavior of any existing functionality that relies on `hatch`. The `pyproject.toml` file has been modified to update the `fmt` and `verify` sections, with `ruff check . --fix` replacing `ruff . --fix` and the removal of `black --check .` and `isort . --check-only`. A new configuration for `isort` has also been added to specify the `databricks.labs.blueprint` package as a known first-party package, enabling more precise management of imports related to that package. These changes simplify the build process and ensure that the project is using a more recent version of the `hatch` package for packaging and distributing Python projects.
* Updated sqlglot requirement from ~=22.3.1 to >=22.3.1,<22.5.0 ([#51](https://github.com/databrickslabs/lsql/issues/51)). In this release, we have updated the version constraint for the `sqlglot` package in our project's `pyproject.toml` file. Previously, we had set the constraint to `~=22.3.1`, allowing for any version with the same major and minor numbers but different patch numbers. With this update, we have changed the constraint to `>=22.3.1,<22.5.0`. This change enables our project to utilize bug fixes and improvements made in the latest patch versions of `sqlglot`, while still preventing it from inadvertently using any breaking changes introduced in version 22.5.0 or later versions. This modification allows us to take advantage of the latest features and improvements in `sqlglot` while maintaining compatibility and stability in our project.

Dependency updates:

 * Updated sqlglot requirement from ~=22.3.1 to >=22.3.1,<22.5.0 ([#51](https://github.com/databrickslabs/lsql/pull/51)).

## 0.2.0

* Added `MockBackend.rows("col1", "col2")[(...), (...)]` helper ([#49](https://github.com/databrickslabs/lsql/issues/49)). In this release, we have added a new helper method `MockBackend.rows("col1", "col2")[(...), (...)]` to simplify testing with `MockBackend`. This method allows for the creation of rows using a more concise syntax, taking in the column names and a list of values to be used for each column, and returning a list of `Row` objects with the specified columns and values. Additionally, a `__eq__` method has been introduced to check if two rows are equal by converting the rows to dictionaries using the existing `as_dict` method and comparing them. The `__contains__` method has also been modified to improve the behavior of the `in` keyword when used with rows, ensuring columns can be checked for membership in the row in a more intuitive and predictable manner. These changes make it easier to test and work with `MockBackend`, improving overall quality and maintainability of the project.


## 0.1.1

* Updated project metadata ([#46](https://github.com/databrickslabs/lsql/issues/46)). In this release, the project metadata has been updated to reflect changes in the library's capabilities and dependencies. The project now supports lightweight SQL statement execution using the Databricks SDK for Python, setting it apart from other solutions. The library size comparison in the documentation has been updated, reflecting an increase in the compressed and uncompressed size of Databricks Labs LightSQL, as well as the addition of a new direct dependency, SQLglot. The project's dependencies and URLs in the `pyproject.toml` file have also been updated, including a version update for `databricks-labs-blueprint` and the removal of a specific range for `PyYAML`.

Dependency updates:

 * Updated sqlglot requirement from ~=22.2.1 to ~=22.3.1 ([#43](https://github.com/databrickslabs/lsql/pull/43)).

## 0.1.0

* Ported `StatementExecutionExt` from UCX ([#31](https://github.com/databrickslabs/lsql/issues/31)).

## 0.0.0

Initial commit
